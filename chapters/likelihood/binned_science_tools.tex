\section{Binned Maximum-Likelihood of \Acstitle{LAT} Data with the Science Tools}
\seclabel{binned_science_tools}

We typically use a binned maximum-likelihood analysis to analyze \ac{LAT}
data.  In this analysis, $\gamm$-rays are binned in position and energy (and
sometimes also separately into front-entering and back-entering events).
The likelihood function comes from the Poisson nature of the observed
emission:
\begin{equation}\eqnlabel{poisson_likelihood_counts_model}
  \likelihood=\prod_j \frac{\theta_j^{k_j} e^{-\theta_j}}{k_j!}.
\end{equation}
Here, $j$ refers to a sum over position and energy bins, $k_j$ are the counts
observed in bin $j$, and $\theta_j$ are the model counts predicted in
the same bin.

The model counts in bin $j$ are computed by integrating the differential
model counts over the bin:
\begin{equation}
  \theta_{ij} = \int_j \intspace \denergy \intspace 
  \dsolidangle \intspace \dtime \intspace 
  \eventrate(\energy,\solidangle,\time|\modelparams_i).
\end{equation}
Here, $j$ represents the integral over the $j$th position/energy
bin, $i$ represents the $i$th source, $\modelparams_i$
refers to the parameters defining the $i$th source, and
$\eventrate(\energy,\solidangle,\time|\modelparams_i)$ is defined in
\eqnref{eventrate}. The total model counts is computed by summing over
all sources:
\begin{equation}
  \theta_j = \sum_i \theta_{ij}
\end{equation}

In most situations, it is more convenient to work with the log of
the likelihood because the log of the likelihood varies more slowly.
In addition, typically a statistical analysis requires either maximizing the
likelihood or looking at a change in the likelihood, which is
arbitrary except for an overall additive constant. So we typically
write the log of the likelihood as
\begin{equation}\eqnlabel{log_likelihood_sum_theta}
  \log\likelihood = -\sum_j\theta_j + \sum_j k_j\log\theta_j 
\end{equation}
where we have dropped the arbitrary additive constant $-\log k_j!$.

In the standard \fermi science tools, \gtbin can be used to perform
basic cuts on the $\gamma$-ray photon list.  The binning of photons
over position in the sky and energy is performed with \gtbin.  The tools
required to compute exposure are \gtltcube and \gtexpcubetwo. Finally,
the likelihood itself is computed with a combination of \gtsrcmaps and
\gtlike.  Essentially, \gtsrcmaps is used to perform the two-dimensional
convolution integral in equation \eqnref{differential_model_counts} and
\gtlike is used to compute the likelihood function defined in equation
\eqnlabel{log_likelihood_sum_theta}.

As we discussed in \secref{description_maximum_likelihood},
we typically use \acp{LRT} to test for significant features in
the $\gamma$-ray data.  For example, we compare a model with and
without a source of interest to test if that source is significant.
\cite{mattox_1996a_likelihood-analysis} shows that for \ac{EGRET} data,
assuming the position of the source was known and that the spectral shape was
fixed, the distribution of \ts in the null hypothesis was
\begin{equation}
  \pdf(\ts) = \tfrac{1}{2} (\delta(\ts) + \chi^2_n(\ts))
\end{equation}
From this, one finds that $\ts^{1/2}$ can be used as a measure of the
statistical significance of the detection of a source.

We finally mention that this formulation assumed that the source models
are time independent.  In principle, these formulas could be generalized
so that the data was binned also in time. But this would almost never be
useful because it is rarely possible to have a simple parameterized model
for the time dependence of a source. Instead, the analysis of a variable
sources is typically performed by dividing the analysis into multiple time
intervals and performing the likelihood fits independently in each time
range. See \cite{nolan_2012_fermi-large} for an example implementation.

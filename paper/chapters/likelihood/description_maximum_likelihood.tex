\section{Description of Maximum-Likelihood Analysis}
\seclabel{description_maximum_likelihood}

The field of $\gamma$-ray astrophysics has generally adopted
maximum-likelihood analysis to avoid the issues discussed in
\secref{motivations_maximum_likelihood}.  The term likelihood
was first introduced by \cite{fisher_1925_statistical-methods}.
Maximum-likelihood was applied to astrophysical photon-counting
experiments by \cite{cash_1979_parameter-estimation}.
\cite{mattox_1996a_likelihood-analysis} described the maximum-likelihood
analysis framework developed to analyze \ac{EGRET} data.

In the formulation, one defines the likelihood, denoted $\likelihood$,
as the probability of obtaining the observed data given an assumed model:
\begin{equation}
  \likelihood = P(\data|\model).
\end{equation}
Generally, a model of the sky is a function of a list of parameters
that we denote as $\modelparams$.  The likelihood function can
be written as:
\begin{equation}
  \likelihood = \likelihood(\modelparams).
\end{equation}
In a maximum-likelihood analysis, one typically fits parameters of a model
by maximizing the likelihood as a function of the parameters of the model.
\begin{equation}
\modelparams_\text{max} = \underset{}{\text{arg }}\underset{\modelparams}{\text{max}} \likelihood(\modelparams)
\end{equation}

Assuming that you have a good model for your data and that
you understand the distribution of the data, maximum-likelihood
analysis can be used to very sensitively test for new features in
your model.  This is because the likelihood function naturally
incorporates data with different significance levels.

Typically, a \ac{LRT} is used to determine the significance of a new
feature in a model. A common use case is searching for a new source
or testing for a spectral break. In a \ac{LRT}, the likelihood under two
hypothesis are compared. We define $\hypothesis_0$ to be a background
model and $\hypothesis_1$ to be a model including the background and in
addition a feature that is being tested for.
Under the assumption that $\hypothesis_0$ is nested
within $\hypothesis_1$, we use Wilks' theorem to
compute the significance of the detection of this feature
\citep{wilks_1938a_large-sample-distribution}.  We define the test
statistic as
\begin{equation}
  \ts = 2\log(\likelihood_{\hypothesis_1}/\likelihood_{\hypothesis_0})
\end{equation}
Here, $\likelihood_{\hypothesis_0}$ and $\likelihood_{\hypothesis_1}$ are the
likelihoods maximized by varying all the parameters of $\hypothesis_0$
and $\hypothesis_1$ respectively.  According to Wilks' theorem,
if $\hypothesis_1$ has $n$ additional degrees of freedom compared to
$\hypothesis_0$, if none of the additional parameters lie on the edge of
parameter space, and if the true data is distributed as $\hypothesis_0$,
then the distribution of \ts should be
\begin{equation}
  \pdf(\ts) = \chi^2_n(\ts)
\end{equation}
Therefore, if one obtains a particular value of \ts, they can use
this this chi-squared distribution to determine the significance of
the detection.
